# Diagnostic DÃ©taillÃ© - Bug "final_score = 0"

**Date :** 21 dÃ©cembre 2025  
**Objectif :** Identifier la cause racine pourquoi tous les items ont final_score = 0.0  
**Statut :** Phase 2 - Diagnostic technique approfondi  

---

## ðŸŽ¯ RÃ‰SUMÃ‰ DU PROBLÃˆME

**SymptÃ´me critique :** Tous les 15 items curated ont `scoring_results.final_score = 0.0`

**DonnÃ©es disponibles :**
- âœ… Module scorer.py existe et semble complet
- âœ… scoring_config bien dÃ©fini dans lai_weekly_v4.yaml
- âœ… Appel scorer.score_items() dans normalization/__init__.py
- âœ… Signaux LAI forts (lai_relevance_score 6-10, matched_domains remplis)

**Question :** Pourquoi le scoring ne fonctionne-t-il pas ?

---

## ðŸ“Š ANALYSE DES DONNÃ‰ES CURATED ACTUELLES

### Patterns ObservÃ©s dans test_curated_items.json

**Groupe 1 : Items avec signaux LAI forts mais final_score = 0**
```json
// Nanexa/Moderna Partnership
{
  "lai_relevance_score": 8,
  "matched_domains": ["tech_lai_ecosystem"],
  "domain_relevance": {"tech_lai_ecosystem": {"score": 0.7}},
  "scoring_results": {
    "base_score": 0.0,        â† PROBLÃˆME
    "bonuses": {},            â† VIDE
    "penalties": {},          â† VIDE
    "final_score": 0.0        â† RÃ‰SULTAT INCORRECT
  }
}

// UZEDY FDA Approval
{
  "lai_relevance_score": 10,
  "matched_domains": ["tech_lai_ecosystem"],
  "domain_relevance": {"tech_lai_ecosystem": {"score": 0.9}},
  "scoring_results": {
    "base_score": 0.0,        â† PROBLÃˆME
    "bonuses": {},            â† VIDE
    "penalties": {},          â† VIDE
    "final_score": 0.0        â† RÃ‰SULTAT INCORRECT
  }
}
```

**Groupe 2 : Items avec pÃ©nalitÃ©s calculÃ©es mais final_score = 0**
```json
// Item avec contenu faible
{
  "lai_relevance_score": 0,
  "matched_domains": [],
  "scoring_results": {
    "base_score": 3.0,        â† CALCULÃ‰
    "penalties": {            â† CALCULÃ‰ES
      "low_lai_score": -3.0,
      "low_relevance_event": -1.0
    },
    "final_score": 0,         â† ARRONDI Ã€ 0
    "score_breakdown": {      â† DÃ‰TAILLÃ‰
      "raw_score": -3.85,
      "scoring_mode": "balanced"
    }
  }
}
```

### Observations Critiques

1. **Deux comportements diffÃ©rents :**
   - Items forts : Tout Ã  0.0 (pas de calcul)
   - Items faibles : Calcul dÃ©taillÃ© mais final_score = 0

2. **Score breakdown prÃ©sent uniquement pour items faibles**
   - SuggÃ¨re que le scoring fonctionne partiellement

3. **Bonuses toujours vides pour items forts**
   - MalgrÃ© entities LAI pertinentes (PharmaShellÂ®, UZEDYÂ®, etc.)

---

## ðŸ” HYPOTHÃˆSES TESTÃ‰ES

### HypothÃ¨se 1 : Fonction de Scoring Non AppelÃ©e âŒ

**Test :** VÃ©rification des appels dans normalization/__init__.py

**RÃ©sultat :** âœ… Appel prÃ©sent ligne 95
```python
scored_items = scorer.score_items(
    matched_items,
    client_config,
    canonical_scopes,
    scoring_mode,
    target_date
)
```

**Conclusion :** La fonction est bien appelÃ©e.

---

### HypothÃ¨se 2 : Configuration scoring_config Manquante âŒ

**Test :** VÃ©rification lai_weekly_v4.yaml

**RÃ©sultat :** âœ… Configuration complÃ¨te prÃ©sente
```yaml
scoring_config:
  event_type_weight_overrides:
    partnership: 8
    regulatory: 7
  client_specific_bonuses:
    pure_player_companies:
      scope: "lai_companies_mvp_core"
      bonus: 5.0
    trademark_mentions:
      scope: "lai_trademarks_global"
      bonus: 4.0
```

**Conclusion :** La configuration est bien dÃ©finie.

---

### HypothÃ¨se 3 : Bug dans l'Algorithme de Scoring âš ï¸

**Test :** Analyse de scorer.py

**Observations :**

1. **Calcul base_score :**
```python
def _get_event_type_score(event_type: str, scoring_config: Dict[str, Any]) -> float:
    default_weights = {
        "partnership": 8.0,
        "regulatory": 7.0,
        # ...
    }
    overrides = scoring_config.get("event_type_weight_overrides", {})
    return overrides.get(event_type, default_weights.get(event_type, 2.0))
```
âœ… Logique correcte

2. **Calcul domain_relevance_factor :**
```python
def _get_domain_relevance_factor(item: Dict[str, Any]) -> float:
    matching_results = item.get("matching_results", {})
    domain_relevance = matching_results.get("domain_relevance", {})
    
    if not domain_relevance:
        return 0.05  # Score trÃ¨s faible si pas de matching
```
ðŸš¨ **PROBLÃˆME POTENTIEL IDENTIFIÃ‰**

3. **Structure domain_relevance dans les donnÃ©es :**
```json
"domain_relevance": {
  "tech_lai_ecosystem": {
    "score": 0.7,
    "confidence": "high"
  }
}
```

4. **Logique de calcul dans scorer.py :**
```python
for domain_id, relevance in domain_relevance.items():
    score = relevance.get("score", 0)
    confidence = relevance.get("confidence", 0)  â† PROBLÃˆME !
```

**ðŸŽ¯ CAUSE RACINE IDENTIFIÃ‰E :**
`confidence` est une string ("high", "medium", "low") mais le code attend un nombre !

---

### HypothÃ¨se 4 : Bug de Conversion confidence âœ…

**Analyse dÃ©taillÃ©e :**

**Dans les donnÃ©es curated :**
```json
"domain_relevance": {
  "tech_lai_ecosystem": {
    "confidence": "high"  â† STRING
  }
}
```

**Dans scorer.py ligne ~200 :**
```python
confidence = relevance.get("confidence", 0)  # RÃ©cupÃ¨re "high"
confidence_scores.append(confidence)         # Ajoute "high" Ã  la liste
avg_confidence = sum(confidence_scores) / len(confidence_scores)  # CRASH !
```

**Erreur :** `sum()` ne peut pas additionner des strings !

**Impact :** Exception dans `_get_domain_relevance_factor()` â†’ domain_relevance_factor = 0.05 â†’ final_score trÃ¨s faible

---

### HypothÃ¨se 5 : Gestion d'Exception MasquÃ©e âœ…

**Dans scorer.py ligne ~50 :**
```python
try:
    scoring_results = _calculate_item_score(...)
except Exception as e:
    logger.error(f"Erreur scoring item {item.get('item_id', 'unknown')}: {str(e)}")
    # Ajout avec score par dÃ©faut
    item["scoring_results"] = _create_default_scoring_result()
```

**Comportement :**
1. Exception dans `_get_domain_relevance_factor()` Ã  cause de confidence string
2. Exception catchÃ©e silencieusement
3. `_create_default_scoring_result()` retourne tout Ã  0.0
4. Item ajoutÃ© avec scoring_results vide

**ðŸŽ¯ CAUSE RACINE CONFIRMÃ‰E :**
Bug de type de donnÃ©es + gestion d'exception masquÃ©e = final_score = 0.0

---

## ðŸ“‹ STATISTIQUES SUR LES 15 ITEMS

### Distribution lai_relevance_score
- **Score 10 :** 3 items (UZEDY, Olanzapine, Delsitech conference)
- **Score 8-9 :** 3 items (Nanexa/Moderna, Medincell grant)
- **Score 6 :** 1 item (Nanexa interim report)
- **Score 0-2 :** 8 items (rapports financiers, corporate moves)

### Distribution matched_domains
- **Avec domains :** 8 items (53%) - tous tech_lai_ecosystem
- **Sans domains :** 7 items (47%) - items non LAI

### Distribution final_score (actuel)
- **Score 0.0 :** 15 items (100%) â† PROBLÃˆME

### Distribution final_score (attendu aprÃ¨s correction)
- **Score >= 12 :** 6-8 items (items LAI forts)
- **Score 8-12 :** 2-3 items (items LAI moyens)
- **Score < 8 :** 4-6 items (items non LAI, exclus newsletter)

---

## ðŸ”§ CAUSE RACINE FINALE

### Bug Principal : Conversion confidence String â†’ Number

**Localisation :** `src_v2/vectora_core/normalization/scorer.py` ligne ~200

**Code dÃ©faillant :**
```python
def _get_domain_relevance_factor(item: Dict[str, Any]) -> float:
    # ...
    for domain_id, relevance in domain_relevance.items():
        score = relevance.get("score", 0)
        confidence = relevance.get("confidence", 0)  # â† BUG: rÃ©cupÃ¨re "high"
        
        relevance_scores.append(score)
        confidence_scores.append(confidence)         # â† BUG: ajoute string
    
    # ...
    avg_confidence = sum(confidence_scores) / len(confidence_scores)  # â† CRASH
```

**DonnÃ©es reÃ§ues :**
```json
"confidence": "high"  // String au lieu de number
```

**Exception gÃ©nÃ©rÃ©e :**
```
TypeError: unsupported operand type(s) for +: 'int' and 'str'
```

**ConsÃ©quence :**
1. Exception dans `_calculate_item_score()`
2. Catch silencieux dans `score_items()`
3. Retour de `_create_default_scoring_result()` (tout Ã  0.0)
4. final_score = 0.0 pour tous les items avec matched_domains

### Bug Secondaire : Gestion d'Exception Trop Large

**Localisation :** `src_v2/vectora_core/normalization/scorer.py` ligne ~50

**ProblÃ¨me :** Exception catchÃ©e silencieusement sans diagnostic

**Impact :** Masque le bug principal, difficile Ã  diagnostiquer

---

## ðŸŽ¯ PLAN DE CORRECTION

### Correction 1 : Mapping confidence String â†’ Number

**Dans `_get_domain_relevance_factor()` :**
```python
def _map_confidence_to_score(confidence_str: str) -> float:
    """Convertit confidence string en score numÃ©rique"""
    mapping = {
        "high": 0.9,
        "medium": 0.6,
        "low": 0.3
    }
    return mapping.get(confidence_str.lower(), 0.5)

# Dans la boucle :
confidence_str = relevance.get("confidence", "medium")
confidence = _map_confidence_to_score(confidence_str)
```

### Correction 2 : AmÃ©lioration Gestion d'Exception

**Dans `score_items()` :**
```python
try:
    scoring_results = _calculate_item_score(...)
except Exception as e:
    logger.error(f"Erreur scoring item {item.get('item_id', 'unknown')}: {str(e)}")
    logger.error(f"DonnÃ©es item: {item.get('matching_results', {})}")  # Debug
    # Retour score par dÃ©faut avec diagnostic
    item["scoring_results"] = _create_default_scoring_result()
    item["scoring_results"]["error"] = str(e)  # TraÃ§abilitÃ©
```

### Correction 3 : Validation des DonnÃ©es d'EntrÃ©e

**Ajout de validation :**
```python
def _validate_matching_results(item: Dict[str, Any]) -> bool:
    """Valide la structure des matching_results"""
    matching_results = item.get("matching_results", {})
    domain_relevance = matching_results.get("domain_relevance", {})
    
    for domain_id, relevance in domain_relevance.items():
        if not isinstance(relevance.get("score"), (int, float)):
            logger.warning(f"Score invalide pour {domain_id}: {relevance.get('score')}")
            return False
        if not isinstance(relevance.get("confidence"), str):
            logger.warning(f"Confidence invalide pour {domain_id}: {relevance.get('confidence')}")
            return False
    
    return True
```

---

## ðŸ“Š VALIDATION POST-CORRECTION

### Test sur Items ReprÃ©sentatifs

**Item 1 - Nanexa/Moderna Partnership :**
- lai_relevance_score: 8
- event_type: "partnership" â†’ base_score: 8.0
- domain_relevance: 0.7 â†’ domain_factor: ~0.7
- entities: ["Nanexa", "Moderna", "PharmaShellÂ®"]
- **final_score attendu :** ~14-16 (base 8 Ã— 0.7 + bonus partnership + bonus trademark)

**Item 2 - UZEDY FDA Approval :**
- lai_relevance_score: 10
- event_type: "regulatory" â†’ base_score: 7.0
- domain_relevance: 0.9 â†’ domain_factor: ~0.9
- entities: ["UZEDYÂ®", "risperidone"]
- **final_score attendu :** ~15-18 (base 7 Ã— 0.9 + bonus regulatory + bonus trademark)

**Item 3 - Rapport financier Nanexa :**
- lai_relevance_score: 0
- event_type: "financial_results" â†’ base_score: 3.0
- matched_domains: [] â†’ domain_factor: 0.05
- **final_score attendu :** ~0-2 (base 3 Ã— 0.05 + pÃ©nalitÃ©s)

---

## ðŸ”„ TRANSITION VERS PHASE 3

**Diagnostic terminÃ© - Cause racine identifiÃ©e :**
1. âœ… **Bug principal :** Conversion confidence string â†’ number
2. âœ… **Bug secondaire :** Gestion d'exception masquÃ©e
3. âœ… **Impact :** final_score = 0.0 pour tous les items avec matched_domains

**Prochaines Ã©tapes :**
- **Phase 3 :** Design scoring V2 propre et config-driven
- **Phase 4 :** ImplÃ©mentation des corrections
- **Phase 5 :** Validation E2E sur lai_weekly_v4

**Corrections prioritaires :**
1. Mapping confidence string â†’ number
2. AmÃ©lioration logging des erreurs
3. Validation des donnÃ©es d'entrÃ©e
4. Tests unitaires sur le scoring

---

*Diagnostic Bug Scoring V2 - Cause Racine IdentifiÃ©e*  
*PrÃªt pour Phase 3 : Design de la correction*