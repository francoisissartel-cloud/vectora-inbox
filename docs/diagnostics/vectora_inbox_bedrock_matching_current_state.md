# Diagnostic Complet - √âtat Actuel de Bedrock dans Vectora Inbox

**Date** : 2025-12-12  
**Objectif** : Diagnostic pr√©cis de l'utilisation de Bedrock aujourd'hui et identification du "trou dans la raquette"  
**Scope** : Analyse compl√®te du pipeline normalisation ‚Üí matching ‚Üí scoring  

---

## üéØ R√©sum√© Ex√©cutif

### Constat Principal
**Bedrock est utilis√© UNIQUEMENT pour la normalisation et la newsletter**, mais **PAS pour le matching/scoring** qui reste 100% d√©terministe. Il existe un **"trou dans la raquette"** entre ce que promet le prompt de normalisation (√©valuation de pertinence LAI) et ce qui est effectivement exploit√© par le matching/scoring.

### Opportunit√© Identifi√©e
Le prompt de normalisation g√©n√®re d√©j√† des champs de pertinence (`lai_relevance_score`, `domain_relevance`) qui ne sont **pas exploit√©s** par le matching/scoring. L'ajout d'un vrai "LLM gating" pourrait se faire en exploitant mieux ces signaux existants ou en ajoutant un prompt d√©di√©.

---

## üìä Phase 1 ‚Äì Diagnostic Pr√©cis de l'√âtat Actuel

### 1.1. Normalisation / Bedrock

#### Ce qui est d√©crit dans `canonical/prompts/global_prompts.yaml`

**Prompt de normalisation LAI** (`normalization.lai_default`) :
- **Extraction d'entit√©s** : companies, molecules, technologies, trademarks, indications
- **Classification d'√©v√©nements** : 9 types (clinical_update, partnership, regulatory, etc.)
- **√âvaluation LAI** : Score 0-10 de pertinence LAI + d√©tection anti-LAI + contexte pure player
- **Technologies LAI hardcod√©es** : 10 technologies sp√©cifiques (Extended-Release Injectable, PLGA, etc.)
- **Trademarks hardcod√©es** : UZEDY, PharmaShell, SiliaShell, BEPO, etc.

#### Ce que construit `_build_normalization_prompt()` dans `bedrock_client.py`

**Version V1 avec prompts canonicalis√©s** :
```python
def _build_normalization_prompt_v1():
    # Feature flag USE_CANONICAL_PROMPTS=true/false
    if use_canonical:
        # Charge depuis canonical/prompts/global_prompts.yaml
        # Substitue {{item_text}}, {{companies_examples}}, {{molecules_examples}}
    else:
        # Fallback vers prompt hardcod√© original
```

**Prompt hardcod√© (fallback)** :
- **Section LAI sp√©cialis√©e** : Technologies et trademarks hardcod√©s
- **10 t√¢ches sp√©cifiques** : Extraction + classification + √©valuation LAI
- **Format JSON rigide** : Structure de r√©ponse fixe
- **Support domaines** : Construction dynamique si `domain_contexts` fourni

#### Ce que Bedrock produit aujourd'hui pour chaque item

**Champs syst√©matiques** :
```json
{
  "summary": "R√©sum√© 2-3 phrases",
  "event_type": "clinical_update|partnership|regulatory|...",
  "companies_detected": ["Nanexa", "Moderna", ...],
  "molecules_detected": ["adalimumab", ...],
  "technologies_detected": ["Extended-Release Injectable", "PLGA", ...],
  "trademarks_detected": ["UZEDY", "PharmaShell", ...],
  "indications_detected": ["schizophrenia", "diabetes", ...],
  "lai_relevance_score": 8,           // ‚ö†Ô∏è G√âN√âR√â MAIS NON EXPLOIT√â
  "anti_lai_detected": false,         // ‚ö†Ô∏è G√âN√âR√â MAIS NON EXPLOIT√â
  "pure_player_context": true         // ‚ö†Ô∏è G√âN√âR√â MAIS NON EXPLOIT√â
}
```

**Champs conditionnels** (si `domain_contexts` fourni) :
```json
{
  "domain_relevance": [               // ‚ö†Ô∏è G√âN√âR√â MAIS NON EXPLOIT√â
    {
      "domain_id": "lai_psychiatry",
      "domain_type": "technology",
      "is_on_domain": true,
      "relevance_score": 0.85,
      "reason": "Article discusses long-acting antipsychotics"
    }
  ]
}
```

### 1.2. Matching (matcher.py)

#### Donn√©es du JSON normalis√© lues par le matcher

**Entit√©s utilis√©es** :
- `companies_detected` ‚Üí intersections avec `company_scope`
- `molecules_detected` ‚Üí intersections avec `molecule_scope`
- `technologies_detected` ‚Üí intersections avec `technology_scope`
- `indications_detected` ‚Üí intersections avec `indication_scope`

**Champs IGNOR√âS** :
- ‚ùå `lai_relevance_score` : **Jamais lu ni utilis√©**
- ‚ùå `anti_lai_detected` : **Jamais lu ni utilis√©**
- ‚ùå `pure_player_context` : **Jamais lu ni utilis√©**
- ‚ùå `domain_relevance` : **Jamais lu ni utilis√©**

#### Logique purement d√©terministe

**Algorithme de matching** :
1. **Extraction des scopes** : Charger les ensembles depuis `canonical_scopes`
2. **Calcul d'intersections** : `item_entities ‚à© scope_entities`
3. **Application des r√®gles** : Depuis `domain_matching_rules.yaml`
4. **Matching contextuel** : Fonction `contextual_matching()` bas√©e sur le type de company

**Exemple de r√®gle** :
```yaml
technology:
  match_mode: "any_required"
  dimensions:
    entity:
      sources: ["company", "molecule"]
      requirement: "required"
      min_matches: 1
    technology:
      requirement: "optional"
      min_matches: 1
```

#### Exploitation du jugement "relevance" de Bedrock

**R√©ponse : AUCUNE**
- Le matching ne lit jamais `lai_relevance_score`
- Le matching ne lit jamais `domain_relevance`
- Seule la fonction `contextual_matching()` utilise `lai_relevance_score` pour filtrer selon le type de company

### 1.3. Scoring (scorer.py)

#### Inputs exacts du scorer

**Champs utilis√©s** :
- `event_type` ‚Üí poids selon l'importance (clinical_update=3, partnership=2.5, etc.)
- `matched_domains` ‚Üí priorit√© du domaine (high/medium/low)
- `companies_detected` ‚Üí bonus pure player
- `date` ‚Üí facteur de r√©cence (d√©croissance exponentielle)
- `source_type` ‚Üí poids selon la source (corporate=2, sector=1.5, generic=1)
- Nombre d'entit√©s ‚Üí bonus profondeur du signal

**Champs IGNOR√âS** :
- ‚ùå `lai_relevance_score` : **Jamais utilis√© dans le scoring**
- ‚ùå `domain_relevance` : **Partiellement support√© mais non utilis√©**
- ‚ùå `anti_lai_detected` : **Jamais utilis√©**

#### Calcul du score final

**Formule actuelle** :
```python
base_score = event_weight * priority_weight * recency_factor * source_weight
final_score = (base_score * confidence_multiplier) + signal_depth_bonus + company_bonus
```

**Champ `domain_relevance` support√© mais non utilis√©** :
```python
def score_items(use_domain_relevance: bool = True):
    if use_domain_relevance and item.get('domain_relevance'):
        # Nouveau syst√®me : utiliser domain_relevance
        score = compute_score_with_domain_relevance(item, ...)
    else:
        # Ancien syst√®me : utiliser matched_domains (UTILIS√â ACTUELLEMENT)
```

#### Moment de rejet/conservation

**Seuils de scoring** : D√©finis dans les r√®gles de scoring, pas dans le code
**Rejet** : Items avec `score < seuil_minimum` (configur√© par client)
**Conservation** : Items avec `score >= seuil_minimum` tri√©s par score d√©croissant

### 1.4. Conclusion Diagnostic

#### √âtapes utilisant r√©ellement Bedrock

1. **Normalisation** : ‚úÖ Utilise Bedrock pour extraction d'entit√©s + √©valuation LAI
2. **Newsletter** : ‚úÖ Utilise Bedrock pour g√©n√©ration √©ditoriale

#### √âtapes 100% d√©terministes

1. **Matching** : ‚ùå Intersections d'ensembles + r√®gles YAML
2. **Scoring** : ‚ùå Formules num√©riques + poids configur√©s

#### Le "Trou dans la Raquette" Identifi√©

**Probl√®me** : Le prompt de normalisation g√©n√®re des signaux de pertinence LAI sophistiqu√©s (`lai_relevance_score`, `domain_relevance`) qui ne sont **jamais exploit√©s** par le matching/scoring.

**Cons√©quences** :
- **Perte d'information** : √âvaluation LLM de la pertinence LAI ignor√©e
- **Matching rigide** : Bas√© uniquement sur des intersections d'entit√©s
- **Scoring simpliste** : Ne tient pas compte du jugement contextuel du LLM
- **Faux positifs** : Items avec entit√©s LAI mais contexte non pertinent
- **Faux n√©gatifs** : Items pertinents LAI sans keywords explicites

**Exemple concret** :
```json
// Item normalis√© par Bedrock
{
  "title": "Nanexa partners with Moderna for PharmaShell technology",
  "companies_detected": ["Nanexa", "Moderna"],
  "technologies_detected": ["PharmaShell"],
  "lai_relevance_score": 9,           // ‚ö†Ô∏è SIGNAL FORT IGNOR√â
  "pure_player_context": true         // ‚ö†Ô∏è CONTEXTE IGNOR√â
}

// Matching actuel : ‚úÖ Match (Nanexa ‚àà pure_players, PharmaShell ‚àà lai_technologies)
// Scoring actuel : Score bas√© sur event_type + pure_player_bonus
// ‚ùå MAIS lai_relevance_score=9 jamais utilis√© pour booster le score
```

---

## üéØ Phase 2 ‚Äì Design Cible : Ajout d'un Vrai Prompt de Matching/Scoring LLM

### 2.1. Architecture Cible Propos√©e

#### Option A : Enrichir le prompt de normalisation (MINIMALE)

**Principe** : Exploiter mieux les champs existants (`lai_relevance_score`, `domain_relevance`)

**Avantages** :
- ‚úÖ Aucun appel Bedrock suppl√©mentaire
- ‚úÖ Signaux d√©j√† disponibles
- ‚úÖ Impl√©mentation rapide

**Inconv√©nients** :
- ‚ùå Prompt de normalisation surcharg√©
- ‚ùå Moins de contr√¥le sur l'√©valuation de pertinence
- ‚ùå M√©lange extraction d'entit√©s et √©valuation de pertinence

#### Option B : Nouveau prompt d√©di√© LLM-matching (RECOMMAND√âE)

**Principe** : Cr√©er un deuxi√®me prompt sp√©cialis√© dans l'√©valuation de pertinence

**Insertion dans le pipeline** :
```
Ingestion ‚Üí Normalisation (Bedrock) ‚Üí LLM-Matching (Bedrock) ‚Üí Matching d√©terministe ‚Üí Scoring ‚Üí Newsletter
```

**Emplacement** : Dans la Lambda `ingest-normalize` apr√®s la normalisation

**Input du prompt LLM-matching** :
```yaml
llm_matching:
  domain_relevance_evaluation:
    user_template: |
      Evaluate the relevance of this biotech/pharma item to specific watch domains.
      
      ORIGINAL TEXT:
      {{item_text_summary}}
      
      NORMALIZED ENTITIES:
      - Companies: {{companies_detected}}
      - Molecules: {{molecules_detected}}
      - Technologies: {{technologies_detected}}
      - Event Type: {{event_type}}
      
      WATCH DOMAINS TO EVALUATE:
      {{watch_domains_context}}
      
      For EACH domain, evaluate:
      1. domain_relevance (0.0-1.0): How relevant is this item to this domain?
      2. is_relevant (true/false): Should this item be included for this domain?
      3. confidence (high/medium/low): Confidence in the evaluation
      4. reasoning (1-2 sentences): Brief explanation
      
      RESPONSE FORMAT (JSON only):
      {
        "domain_evaluations": [
          {
            "domain_id": "lai_psychiatry",
            "domain_relevance": 0.85,
            "is_relevant": true,
            "confidence": "high",
            "reasoning": "Article discusses partnership for long-acting antipsychotic development"
          }
        ]
      }
```

**Output du prompt LLM-matching** :
```json
{
  "domain_evaluations": [
    {
      "domain_id": "lai_psychiatry",
      "domain_relevance": 0.85,
      "is_relevant": true,
      "confidence": "high",
      "reasoning": "Partnership between pure player and Big Pharma for LAI antipsychotic"
    },
    {
      "domain_id": "lai_diabetes",
      "domain_relevance": 0.1,
      "is_relevant": false,
      "confidence": "high",
      "reasoning": "No mention of diabetes or metabolic indications"
    }
  ]
}
```

### 2.2. G√©n√©ralisation Multi-Technologies

#### Design g√©n√©rique

**Pas de hardcoding LAI** :
- Technologies depuis `canonical/scopes/technologies/`
- Watch domains depuis `client_config`
- Prompts adaptables par vertical

**Configuration par client** :
```yaml
# client_config/lai_weekly_v3.yaml
watch_domains:
  - id: "lai_psychiatry"
    type: "technology"
    technology_scope: "lai_technologies_psychiatry"
    company_scope: "lai_companies_pure_players"
    priority: "high"
  - id: "lai_diabetes"
    type: "technology"
    technology_scope: "lai_technologies_diabetes"
    company_scope: "lai_companies_hybrid"
    priority: "medium"
```

**Template g√©n√©rique** :
```yaml
llm_matching:
  domain_relevance_evaluation:
    user_template: |
      Evaluate relevance to {{client_vertical}} domains:
      
      DOMAINS: {{watch_domains_descriptions}}
      TECHNOLOGIES: {{technology_scopes_examples}}
      COMPANIES: {{company_scopes_examples}}
      
      Item: {{item_summary}}
      
      Evaluate each domain (0.0-1.0 relevance)...
```

#### Support multi-vertical

**Oncologie** :
```yaml
watch_domains:
  - id: "car_t_therapy"
    technology_scope: "car_t_technologies"
  - id: "immunotherapy"
    technology_scope: "immunotherapy_technologies"
```

**Cell Therapy** :
```yaml
watch_domains:
  - id: "stem_cell_therapy"
    technology_scope: "stem_cell_technologies"
  - id: "gene_therapy"
    technology_scope: "gene_therapy_technologies"
```

### 2.3. Co√ªt et Performances

#### Estimation des appels Bedrock suppl√©mentaires

**Run lai_weekly_v3 typique** :
- Items ing√©r√©s : ~300-500
- Items normalis√©s : ~100-200 (apr√®s filtrage sources)
- **Appels LLM-matching suppl√©mentaires : +100-200**

**Impact co√ªt approximatif** :
- Co√ªt normalisation actuel : ~$2-4 par run
- **Co√ªt LLM-matching suppl√©mentaire : +$1-2 par run**
- **Augmentation : +25-50% du co√ªt Bedrock**

#### Risques de throttling

**R√©gion us-east-1** (normalisation) :
- Limite actuelle : ~100 req/min
- Avec LLM-matching : ~200 req/min
- **Risque : MOYEN** (proche des limites)

**Mitigation** :
- Batch processing (5-10 items par appel)
- Retry avec backoff exponentiel
- R√©gion eu-west-3 pour LLM-matching

#### Pistes pour contr√¥ler le co√ªt

**Pr√©filtre d√©terministe** :
```python
def should_call_llm_matching(item):
    # Appeler LLM-matching seulement si :
    # 1. Au moins 1 entit√© d√©tect√©e
    # 2. Event type pertinent
    # 3. Pas de signaux anti-LAI √©vidents
    return (
        len(item.get('companies_detected', [])) > 0 or
        len(item.get('technologies_detected', [])) > 0
    ) and item.get('event_type') != 'financial_results'
```

**Seuils adaptatifs** :
- Items avec `lai_relevance_score < 3` : Skip LLM-matching
- Items avec `anti_lai_detected = true` : Skip LLM-matching
- Items avec event_type = 'financial_results' : Skip LLM-matching

**Batch processing** :
```python
# Traiter 5 items par appel Bedrock
def batch_llm_matching(items_batch):
    prompt = build_batch_matching_prompt(items_batch)
    # 1 appel pour 5 items = -80% d'appels
```

---

## üöÄ Phase 3 ‚Äì Plan de Mise en ≈íuvre

### Phase A : Enrichir le prompt de normalisation (OPTION MINIMALE)

**Objectif** : Exploiter mieux les champs existants sans appels Bedrock suppl√©mentaires

**Actions** :
1. **Modifier le matcher** : Lire `lai_relevance_score` et `domain_relevance`
2. **Modifier le scorer** : Utiliser `compute_score_with_domain_relevance()`
3. **Ajuster les seuils** : Filtrer items avec `lai_relevance_score < 5`

**Avantages** :
- ‚úÖ Impl√©mentation rapide (1-2 jours)
- ‚úÖ Aucun co√ªt suppl√©mentaire
- ‚úÖ Am√©lioration imm√©diate

**Inconv√©nients** :
- ‚ùå Am√©lioration limit√©e
- ‚ùå Prompt de normalisation surcharg√©

### Phase B : Cr√©er un nouveau prompt d√©di√© LLM-matching (OPTION RECOMMAND√âE)

**Objectif** : Vrai "LLM gating" avec prompt sp√©cialis√©

#### Phase B1 : Design et impl√©mentation (3-4 jours)

**Actions** :
1. **Cr√©er le prompt** : `canonical/prompts/global_prompts.yaml`
2. **√âtendre PromptLoader** : Support du nouveau prompt
3. **Modifier bedrock_client** : Nouvelle fonction `evaluate_domain_relevance_with_bedrock()`
4. **Int√©grer dans ingest-normalize** : Appel apr√®s normalisation

**Prompt LLM-matching** :
```yaml
matching:
  domain_relevance_evaluation:
    system_instructions: |
      You are a domain relevance expert for biotech/pharma intelligence.
      Evaluate how relevant news items are to specific technology domains.
      
    user_template: |
      Evaluate this item's relevance to watch domains:
      
      ITEM SUMMARY: {{item_summary}}
      ENTITIES: Companies={{companies}}, Technologies={{technologies}}
      EVENT TYPE: {{event_type}}
      
      DOMAINS TO EVALUATE:
      {{domains_context}}
      
      For each domain, provide:
      - relevance_score (0.0-1.0)
      - is_relevant (true/false)  
      - confidence (high/medium/low)
      - reasoning (1-2 sentences)
      
      JSON format only.
```

#### Phase B2 : Adapter le matching (1-2 jours)

**Modifier matcher.py** :
```python
def match_items_to_domains_with_llm(normalized_items, watch_domains, ...):
    for item in normalized_items:
        # 1. Matching d√©terministe (existant)
        deterministic_matches = compute_deterministic_matches(item, ...)
        
        # 2. LLM matching (nouveau)
        llm_evaluations = item.get('domain_evaluations', [])
        llm_matches = [e['domain_id'] for e in llm_evaluations if e['is_relevant']]
        
        # 3. Combinaison (ET logique ou OU logique selon config)
        if matching_mode == 'llm_only':
            item['matched_domains'] = llm_matches
        elif matching_mode == 'deterministic_and_llm':
            item['matched_domains'] = list(set(deterministic_matches) & set(llm_matches))
        else:  # 'deterministic_or_llm'
            item['matched_domains'] = list(set(deterministic_matches) | set(llm_matches))
```

#### Phase B3 : Adapter le scoring (1-2 jours)

**Modifier scorer.py** :
```python
def compute_score_with_llm_relevance(item, scoring_rules, ...):
    # Utiliser domain_evaluations pour le scoring
    domain_evaluations = item.get('domain_evaluations', [])
    
    # Prendre le meilleur score de pertinence
    max_relevance = max([e.get('relevance_score', 0) for e in domain_evaluations])
    
    # Bonus de confiance
    high_confidence_count = len([e for e in domain_evaluations if e.get('confidence') == 'high'])
    
    # Formule adapt√©e
    base_score = event_weight * max_relevance * recency_factor
    confidence_bonus = high_confidence_count * 0.5
    
    return base_score + confidence_bonus + other_bonuses
```

### Phase C : Tests et validation (2-3 jours)

**Dataset de r√©f√©rence** :
- Nanexa/Moderna partnership (LAI-strong)
- UZEDY regulatory updates (LAI-strong)
- MedinCell malaria program (LAI-strong)
- Pfizer financial results (LAI-weak)
- Generic biotech hiring (LAI-irrelevant)

**M√©triques de validation** :
- **Pr√©cision** : % d'items pertinents dans les r√©sultats
- **Rappel** : % d'items pertinents LAI d√©tect√©s
- **F1-score** : √âquilibre pr√©cision/rappel
- **Co√ªt** : Nombre d'appels Bedrock suppl√©mentaires

### Phase D : D√©ploiement AWS (1 jour)

**Actions** :
1. **D√©ploiement DEV** : Lambda ingest-normalize avec LLM-matching
2. **Configuration** : Feature flag `USE_LLM_MATCHING=true`
3. **Test E2E** : Run lai_weekly_v3 complet
4. **Monitoring** : Logs et m√©triques CloudWatch

### Phase E : Validation et optimisation (1-2 jours)

**Validation qualit√©** :
- Comparaison avant/apr√®s sur dataset historique
- Analyse des faux positifs/n√©gatifs
- Ajustement des seuils de pertinence

**Optimisation co√ªt** :
- Impl√©mentation du pr√©filtre
- Batch processing si n√©cessaire
- Monitoring des co√ªts Bedrock

---

## üìã Contraintes et Recommandations

### Contraintes Respect√©es

- ‚úÖ **Aucune modification** du code ni des prompts √† cette √©tape
- ‚úÖ **Analyse bas√©e** sur le code, canonical, clients et docs existants
- ‚úÖ **Pas de simulation** de runs

### Recommandations Finales

#### Recommandation Principale : **Option B (Nouveau prompt LLM-matching)**

**Justification** :
- **Impact m√©tier √©lev√©** : Vrai "LLM gating" pour am√©liorer la pertinence
- **Architecture propre** : S√©paration extraction d'entit√©s / √©valuation de pertinence
- **√âvolutivit√©** : Support multi-vertical et personnalisation client
- **ROI acceptable** : +25-50% co√ªt Bedrock pour am√©lioration qualit√© significative

#### Plan de D√©ploiement Recommand√©

1. **Phase A (Quick Win)** : Exploiter `lai_relevance_score` existant (1-2 jours)
2. **Phase B (Solution cible)** : Nouveau prompt LLM-matching (1 semaine)
3. **Phase C (Optimisation)** : Pr√©filtre et batch processing (2-3 jours)

#### M√©triques de Succ√®s

- **Qualit√©** : +20% de pr√©cision sur items LAI-strong
- **Co√ªt** : <+50% du budget Bedrock actuel
- **Performance** : <+30% de latence sur le pipeline complet

---

**Ce diagnostic r√©v√®le un potentiel d'am√©lioration significatif en exploitant mieux les capacit√©s LLM pour le matching/scoring, avec un ROI favorable et une architecture √©volutive.**