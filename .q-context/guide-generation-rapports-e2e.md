# Guide: GÃ©nÃ©ration de Rapports E2E Tests

**Objectif**: GÃ©nÃ©rer un rapport E2E complet avec mÃ©triques opÃ©rationnelles pour piloter le workflow Vectora Inbox.

## ğŸ“‹ Format de RÃ©fÃ©rence

**Fichier golden standard**: `test-e2e-gold-standard.md`

Ce fichier est le **modÃ¨le exact** Ã  reproduire pour tout nouveau test E2E.

## ğŸ¯ Quand GÃ©nÃ©rer un Rapport E2E

- AprÃ¨s chaque run de test complet (ingest â†’ normalize-score â†’ newsletter)
- AprÃ¨s modification des prompts de normalisation ou domain scoring
- AprÃ¨s changement de version des Lambdas
- Pour valider une nouvelle version client (ex: lai_weekly_v24, v25, etc.)

## ğŸ“ Structure du Rapport (Ã€ Reproduire Exactement)

### 1. En-tÃªte
```markdown
# Rapport DÃ©taillÃ© E2E - {client_id} {ENV}

**Date**: YYYY-MM-DD
**Client**: {client_id}
**Environnement**: {env}
```

### 2. MÃ©triques de Performance âš¡
- Temps d'exÃ©cution par phase (ingest, normalize-score, newsletter)
- Throughput (items/seconde)
- Temps moyen par item

### 3. MÃ©triques Bedrock ğŸ¤–
- Nombre d'appels API (total, normalization, scoring)
- Consommation tokens (input, output)
- CoÃ»ts unitaires (par item, par appel, par item pertinent)

### 4. VolumÃ©trie DÃ©taillÃ©e ğŸ“Š
- Pipeline complet: ingestion â†’ normalisation â†’ scoring â†’ filtrage
- Taux de conversion Ã  chaque Ã©tape

### 5. Projections CoÃ»ts ğŸ’°
- Par frÃ©quence d'exÃ©cution (hebdo, quotidien, 2x/jour)
- Par volume d'items (50, 100, 500)

### 6. KPIs Pilotage ğŸ¯
- Performance (temps E2E, throughput, disponibilitÃ©)
- QualitÃ© (taux normalisation, taux pertinence, score moyen)
- CoÃ»ts (coÃ»t/item, coÃ»t/run, coÃ»t mensuel)
- Recommandations

### 7. Statistiques Globales ğŸ“Š
- Total items, items pertinents, items non-pertinents
- Score moyen, min, max

### 8. Items Pertinents (DÃ©tail) âœ…
Pour chaque item pertinent:
```markdown
### Item X/Y

**Titre**: {title}
**Source**: {source}
**Date**: {date}
**URL**: {url}

#### ğŸ“ Normalisation (1er appel Bedrock)
**Summary**: {summary}
**EntitÃ©s dÃ©tectÃ©es**:
- Companies: {list}
- Technologies: {list}
- Molecules: {list}
- Trademarks: {list}
- Indications: {list}
- Dosing intervals: {list}
**Event type**: {type}

#### ğŸ¯ Domain Scoring (2Ã¨me appel Bedrock)
**Score**: {score}/100
**Confidence**: {confidence}
**Is relevant**: {true/false}
**Signaux dÃ©tectÃ©s**:
- Strong: {list}
- Medium: {list}
- Weak: {list}
- Exclusions: {list}
**Score breakdown**: {dÃ©tail calcul}
**Reasoning**: {explication}
```

### 9. Items Non-Pertinents (RÃ©sumÃ©) âŒ
Pour chaque item non-pertinent:
```markdown
### Item X/Y

**Titre**: {title}
**Source**: {source}
**Date**: {date}
**EntitÃ©s dÃ©tectÃ©es**: {rÃ©sumÃ©}
**Raison du rejet**: {reasoning}
```

### 10. Analyse par CatÃ©gorie ğŸ”
- Par type d'Ã©vÃ©nement
- Par signal fort dÃ©tectÃ©

## ğŸ› ï¸ Comment GÃ©nÃ©rer le Rapport

### Ã‰tape 1: Extraire les DonnÃ©es

**Source des donnÃ©es**:
- Fichier curated: `tests/data_snapshots/{client_id}_curated.json`
- Logs CloudWatch (optionnel pour mÃ©triques temps rÃ©el)

**Script de rÃ©fÃ©rence**: `.tmp/generate_detailed_report.py`

### Ã‰tape 2: Calculer les MÃ©triques

**MÃ©triques estimÃ©es** (si logs CloudWatch indisponibles):
```python
metrics = {
    'ingest_duration_ms': 2500,  # ~2.5s pour 32 items
    'normalize_duration_ms': 95000,  # ~95s pour 64 appels Bedrock
    'newsletter_duration_ms': 1500,  # ~1.5s
    'items_ingested': len(items),
    'items_normalized': len([i for i in items if 'normalized_content' in i]),
    'items_relevant': len([i for i in items if i.get('domain_scoring', {}).get('is_relevant')]),
    'bedrock_calls': items_normalized * 2,  # 2 appels par item
    'input_tokens': bedrock_calls * 3000,  # ~3K tokens/appel
    'output_tokens': bedrock_calls * 500   # ~500 tokens/appel
}
```

**Pricing Bedrock**:
- Input: $0.003 / 1K tokens
- Output: $0.015 / 1K tokens

### Ã‰tape 3: GÃ©nÃ©rer le Markdown

**Template**: Utiliser `test-e2e-gold-standard.md` comme rÃ©fÃ©rence exacte

**Sections obligatoires**:
1. âš¡ MÃ©triques de Performance
2. ğŸ¤– MÃ©triques Bedrock
3. ğŸ“Š VolumÃ©trie DÃ©taillÃ©e
4. ğŸ’° Projections CoÃ»ts
5. ğŸ¯ KPIs Pilotage
6. ğŸ“Š Statistiques Globales
7. âœ… Items Pertinents (dÃ©tail complet)
8. âŒ Items Non-Pertinents (rÃ©sumÃ©)
9. ğŸ” Analyse par CatÃ©gorie

## ğŸ“¦ Livrables

Pour chaque test E2E, gÃ©nÃ©rer:

1. **Rapport complet**: `test_e2e_{client_id}_rapport_detaille_{date}_avec_metriques.md`
2. **MÃ©triques JSON**: `test_e2e_{client_id}_metriques_{date}.json`
3. **Golden test data**: `tests/data_snapshots/golden_test_{client_id}_{date}.json`

## ğŸ¯ Prompt pour Q Developer

```
GÃ©nÃ¨re un rapport E2E complet pour le client {client_id} en utilisant le format 
exact de test-e2e-gold-standard.md. 

DonnÃ©es source: tests/data_snapshots/{client_id}_curated.json

Le rapport doit inclure:
- MÃ©triques de performance (temps, throughput)
- MÃ©triques Bedrock (appels, tokens, coÃ»ts)
- VolumÃ©trie dÃ©taillÃ©e
- Projections coÃ»ts
- KPIs pilotage
- DÃ©tail de tous les items pertinents
- RÃ©sumÃ© des items non-pertinents
- Analyse par catÃ©gorie

Utilise les mÃªmes sections, emojis, et structure que le golden standard.
```

## ğŸ“š Fichiers de RÃ©fÃ©rence

- **Golden standard**: `.q-context/test-e2e-gold-standard.md`
- **Script gÃ©nÃ©ration**: `.tmp/generate_detailed_report.py`
- **Script enrichissement**: `.tmp/enrich_report_with_metrics.py`
- **Index rapports**: `docs/reports/e2e/INDEX_RAPPORTS_V23.md`

---

**Note**: Ce format est validÃ© et utilisÃ© pour le pilotage opÃ©rationnel. Ne pas modifier la structure sans validation.
