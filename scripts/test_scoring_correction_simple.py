#!/usr/bin/env python3
"""
Test simplifi√© de la correction du bug confidence string ‚Üí number
"""

import json
from datetime import datetime

def test_confidence_mapping():
    """Test du mapping confidence string ‚Üí number"""
    print("üîß Test Correction Bug Confidence")
    print("=" * 40)
    
    # Simulation de la fonction corrig√©e
    def get_domain_relevance_factor_corrected(item):
        """Version corrig√©e de _get_domain_relevance_factor"""
        matching_results = item.get("matching_results", {})
        domain_relevance = matching_results.get("domain_relevance", {})
        
        if not domain_relevance:
            return 0.05
        
        relevance_scores = []
        confidence_scores = []
        
        for domain_id, relevance in domain_relevance.items():
            score = relevance.get("score", 0)
            confidence_str = relevance.get("confidence", "medium")
            
            # CORRECTION: Mapping confidence string ‚Üí number
            confidence_mapping = {
                "high": 0.9,
                "medium": 0.6,
                "low": 0.3
            }
            confidence = confidence_mapping.get(confidence_str.lower(), 0.5)
            
            relevance_scores.append(score)
            confidence_scores.append(confidence)
        
        if not relevance_scores:
            return 0.3
        
        # Calcul sans erreur
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        final_factor = (avg_relevance * 0.6) + (avg_confidence * 0.3)
        
        return min(1.0, final_factor)
    
    # Test avec donn√©es r√©elles du dataset
    test_cases = [
        {
            "name": "Nanexa/Moderna Partnership",
            "item": {
                "matching_results": {
                    "domain_relevance": {
                        "tech_lai_ecosystem": {
                            "score": 0.7,
                            "confidence": "high"  # String
                        }
                    }
                }
            },
            "expected_factor": 0.69  # (0.7 * 0.6) + (0.9 * 0.3) = 0.42 + 0.27 = 0.69
        },
        {
            "name": "UZEDY FDA Approval", 
            "item": {
                "matching_results": {
                    "domain_relevance": {
                        "tech_lai_ecosystem": {
                            "score": 0.9,
                            "confidence": "high"  # String
                        }
                    }
                }
            },
            "expected_factor": 0.81  # (0.9 * 0.6) + (0.9 * 0.3) = 0.54 + 0.27 = 0.81
        },
        {
            "name": "Item sans matching",
            "item": {
                "matching_results": {
                    "domain_relevance": {}
                }
            },
            "expected_factor": 0.05
        }
    ]
    
    success_count = 0
    
    for test_case in test_cases:
        print(f"\nüìã Test: {test_case['name']}")
        
        try:
            # Test de la fonction corrig√©e
            result = get_domain_relevance_factor_corrected(test_case["item"])
            expected = test_case["expected_factor"]
            
            print(f"   R√©sultat: {result:.3f}")
            print(f"   Attendu:  {expected:.3f}")
            
            if abs(result - expected) < 0.01:  # Tol√©rance de 0.01
                print("   ‚úÖ PASS")
                success_count += 1
            else:
                print("   ‚ùå FAIL - Diff√©rence trop importante")
                
        except Exception as e:
            print(f"   ‚ùå ERREUR: {e}")
    
    print(f"\nüéØ R√©sultats: {success_count}/{len(test_cases)} tests r√©ussis")
    return success_count == len(test_cases)

def test_scoring_calculation():
    """Test du calcul de scoring complet"""
    print("\nüßÆ Test Calcul Scoring Complet")
    print("=" * 40)
    
    # Simulation simplifi√©e du scoring
    def calculate_simple_score(item):
        """Calcul de score simplifi√© pour test"""
        normalized = item.get("normalized_content", {})
        lai_score = normalized.get("lai_relevance_score", 0)
        event_type = normalized.get("event_classification", {}).get("primary_type", "other")
        
        # Poids event types
        event_weights = {
            "partnership": 1.2,
            "regulatory": 1.1,
            "clinical_update": 1.0,
            "financial_results": 0.5,
            "other": 0.3
        }
        
        # Score de base
        base_score = lai_score * event_weights.get(event_type, 0.5)
        
        # Facteur domaine (corrig√©)
        matching_results = item.get("matching_results", {})
        domain_relevance = matching_results.get("domain_relevance", {})
        
        if domain_relevance:
            # Simulation du calcul corrig√©
            for domain_id, relevance in domain_relevance.items():
                score = relevance.get("score", 0)
                confidence_str = relevance.get("confidence", "medium")
                
                confidence_mapping = {"high": 0.9, "medium": 0.6, "low": 0.3}
                confidence = confidence_mapping.get(confidence_str.lower(), 0.5)
                
                domain_factor = (score * 0.6) + (confidence * 0.3)
                break  # Premier domaine pour simplification
        else:
            domain_factor = 0.05
        
        # Score final
        final_score = base_score * domain_factor
        
        # Bonus LAI score √©lev√©
        if lai_score >= 8:
            final_score += 2.0
        elif lai_score >= 6:
            final_score += 1.0
        
        return max(0, final_score)
    
    # Test avec items r√©els
    test_items = [
        {
            "name": "Nanexa/Moderna (LAI fort)",
            "item": {
                "normalized_content": {
                    "lai_relevance_score": 8,
                    "event_classification": {"primary_type": "partnership"}
                },
                "matching_results": {
                    "domain_relevance": {
                        "tech_lai_ecosystem": {"score": 0.7, "confidence": "high"}
                    }
                }
            },
            "expected_min": 8.0  # Devrait √™tre s√©lectionnable
        },
        {
            "name": "UZEDY FDA (LAI tr√®s fort)",
            "item": {
                "normalized_content": {
                    "lai_relevance_score": 10,
                    "event_classification": {"primary_type": "regulatory"}
                },
                "matching_results": {
                    "domain_relevance": {
                        "tech_lai_ecosystem": {"score": 0.9, "confidence": "high"}
                    }
                }
            },
            "expected_min": 10.0  # Devrait √™tre excellent
        },
        {
            "name": "Rapport financier (non LAI)",
            "item": {
                "normalized_content": {
                    "lai_relevance_score": 0,
                    "event_classification": {"primary_type": "financial_results"}
                },
                "matching_results": {"domain_relevance": {}}
            },
            "expected_max": 1.0  # Devrait √™tre exclu
        }
    ]
    
    success_count = 0
    
    for test_item in test_items:
        print(f"\nüìã Test: {test_item['name']}")
        
        try:
            score = calculate_simple_score(test_item["item"])
            print(f"   Score calcul√©: {score:.2f}")
            
            if "expected_min" in test_item:
                if score >= test_item["expected_min"]:
                    print("   ‚úÖ PASS - Score suffisant")
                    success_count += 1
                else:
                    print(f"   ‚ùå FAIL - Score trop faible (min: {test_item['expected_min']})")
            elif "expected_max" in test_item:
                if score <= test_item["expected_max"]:
                    print("   ‚úÖ PASS - Score appropri√©")
                    success_count += 1
                else:
                    print(f"   ‚ùå FAIL - Score trop √©lev√© (max: {test_item['expected_max']})")
                    
        except Exception as e:
            print(f"   ‚ùå ERREUR: {e}")
    
    print(f"\nüéØ R√©sultats: {success_count}/{len(test_items)} tests r√©ussis")
    return success_count == len(test_items)

def analyze_test_dataset():
    """Analyse du dataset de test avec la correction"""
    print("\nüìä Analyse Dataset test_curated_items.json")
    print("=" * 40)
    
    try:
        with open("test_curated_items.json", 'r', encoding='utf-8') as f:
            items = json.load(f)
        
        print(f"Items charg√©s: {len(items)}")
        
        # Analyse des items avec matched_domains
        items_with_domains = []
        items_without_domains = []
        
        for item in items:
            matching_results = item.get("matching_results", {})
            matched_domains = matching_results.get("matched_domains", [])
            
            if matched_domains:
                items_with_domains.append(item)
            else:
                items_without_domains.append(item)
        
        print(f"\nüìà R√©partition:")
        print(f"   Items avec matched_domains: {len(items_with_domains)}")
        print(f"   Items sans matched_domains: {len(items_without_domains)}")
        
        # Analyse des confidence values
        confidence_values = {}
        for item in items_with_domains:
            domain_relevance = item.get("matching_results", {}).get("domain_relevance", {})
            for domain_id, relevance in domain_relevance.items():
                confidence = relevance.get("confidence", "unknown")
                confidence_values[confidence] = confidence_values.get(confidence, 0) + 1
        
        print(f"\nüîç Valeurs confidence d√©tect√©es:")
        for conf, count in confidence_values.items():
            print(f"   '{conf}': {count} occurrences")
        
        # Simulation de l'impact de la correction
        print(f"\nüéØ Impact attendu de la correction:")
        print(f"   Items qui auraient caus√© l'erreur: {len(items_with_domains)}")
        print(f"   Items qui fonctionnaient d√©j√†: {len(items_without_domains)}")
        
        # Estimation des scores apr√®s correction
        high_lai_items = []
        for item in items:
            lai_score = item.get("normalized_content", {}).get("lai_relevance_score", 0)
            if lai_score >= 6:
                high_lai_items.append(item)
        
        print(f"   Items LAI pertinents (score >= 6): {len(high_lai_items)}")
        print(f"   Taux de s√©lection attendu: {len(high_lai_items)/len(items)*100:.1f}%")
        
        return True
        
    except FileNotFoundError:
        print("‚ùå Fichier test_curated_items.json non trouv√©")
        return False
    except Exception as e:
        print(f"‚ùå Erreur analyse: {e}")
        return False

def main():
    """Fonction principale"""
    print("VALIDATION CORRECTION SCORING V2")
    print("=" * 50)
    
    # Tests
    test1 = test_confidence_mapping()
    test2 = test_scoring_calculation() 
    test3 = analyze_test_dataset()
    
    # R√©sum√©
    print("\n" + "=" * 50)
    print("RESUME VALIDATION")
    print(f"   Test mapping confidence: {'PASS' if test1 else 'FAIL'}")
    print(f"   Test calcul scoring: {'PASS' if test2 else 'FAIL'}")
    print(f"   Analyse dataset: {'PASS' if test3 else 'FAIL'}")
    
    overall_success = test1 and test2 and test3
    
    print(f"\nRESULTAT: {'CORRECTION VALIDEE' if overall_success else 'TESTS ECHOUES'}")
    
    if overall_success:
        print("\nLa correction du bug confidence est fonctionnelle!")
        print("   -> Le mapping string -> number fonctionne")
        print("   -> Les items LAI auront des final_score > 0")
        print("   -> Pret pour test E2E sur normalize_score_v2")
    
    return 0 if overall_success else 1

if __name__ == "__main__":
    exit(main())